{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import midi\n",
    "import wave\n",
    "import scipy\n",
    "import struct\n",
    "import librosa\n",
    "import numpy as np\n",
    "from mido import MidiFile\n",
    "import pydub.scipy_effects\n",
    "from pydub import AudioSegment\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.fft import fft, ifft\n",
    "from IPython.display import Audio\n",
    "from scipy.io.wavfile import read, write\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import patches\n",
    "from matplotlib import colors\n",
    "import pretty_midi\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('eod.wav')\n",
    "pattern = MidiFile('EdgeofDesire.mid', clip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs, data = read('eod.wav')\n",
    "data = data[:,0]\n",
    "print(\"Sampling Frequency is\", Fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data, rate=Fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Waveform of Test Audio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
    "ftr = librosa.frames_to_time(beats, sr=sr)\n",
    "onset_env = librosa.onset.onset_strength(y, sr=sr, aggregate=np.median)\n",
    "tempo1, beats1 = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "hop_length = 512\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "times = librosa.times_like(onset_env, sr=sr, hop_length=hop_length)\n",
    "M = librosa.feature.melspectrogram(y=y, sr=sr, hop_length=hop_length)\n",
    "librosa.display.specshow(librosa.power_to_db(M, ref=np.max),\n",
    "                         y_axis='mel', x_axis='time', hop_length=hop_length,\n",
    "                         ax=ax[0])\n",
    "ax[0].label_outer()\n",
    "ax[0].set(title='Mel spectrogram')\n",
    "ax[1].plot(times, librosa.util.normalize(onset_env),\n",
    "         label='Onset strength')\n",
    "ax[1].vlines(times[beats1], 0, 1, alpha=0.5, color='r',\n",
    "           linestyle='--', label='Beats')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = librosa.stft(y)\n",
    "D_harmonic, D_percussive = librosa.decompose.hpss(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute a global reference power from the input spectrum\n",
    "rp = np.max(np.abs(D))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, sharex=True, sharey=True)\n",
    "\n",
    "img = librosa.display.specshow(librosa.amplitude_to_db(np.abs(D), ref=rp),\n",
    "                         y_axis='log', x_axis='time', ax=ax[0])\n",
    "ax[0].set(title='Full spectrogram')\n",
    "ax[0].label_outer()\n",
    "\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(D_harmonic), ref=rp),\n",
    "                         y_axis='log', x_axis='time', ax=ax[1])\n",
    "ax[1].set(title='Harmonic spectrogram')\n",
    "ax[1].label_outer()\n",
    "\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(D_percussive), ref=rp),\n",
    "                         y_axis='log', x_axis='time', ax=ax[2])\n",
    "ax[2].set(title='Percussive spectrogram')\n",
    "fig.colorbar(img, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATE NOTES FROM USER'S AUDIO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from pydub.utils import get_array_type\n",
    "#from Levenshtein import distance\n",
    "\n",
    "NOTES = {\n",
    "    \"A\": 440,\n",
    "    \"A#\": 466.1637615180899,\n",
    "    \"B\": 493.8833012561241,\n",
    "    \"C\": 523.2511306011972,\n",
    "    \"C#\": 554.3652619537442,\n",
    "    \"D\": 587.3295358348151,\n",
    "    \"D#\": 622.2539674441618,\n",
    "    \"E\": 659.2551138257398,\n",
    "    \"F\": 698.4564628660078,\n",
    "    \"F#\": 739.9888454232688,\n",
    "    \"G\": 783.9908719634985,\n",
    "    \"G#\": 830.6093951598903,\n",
    "}\n",
    "\n",
    "\n",
    "def frequency_spectrum(sample, max_frequency=800):\n",
    "    \"\"\"\n",
    "    Derive frequency spectrum of a signal pydub.AudioSample\n",
    "    Returns an array of frequencies and an array of how prevelant that frequency is in the sample\n",
    "    \"\"\"\n",
    "    # Convert pydub.AudioSample to raw audio data\n",
    "    # Copied from Jiaaro's answer on https://stackoverflow.com/questions/32373996/pydub-raw-audio-data\n",
    "    bit_depth = sample.sample_width * 8\n",
    "    array_type = get_array_type(bit_depth)\n",
    "    raw_audio_data = array.array(array_type, sample._data)\n",
    "    n = len(raw_audio_data)\n",
    "\n",
    "    # Compute FFT and frequency value for each index in FFT array\n",
    "    # Inspired by Reveille's answer on https://stackoverflow.com/questions/53308674/audio-frequencies-in-python\n",
    "    freq_array = np.arange(n) * (float(sample.frame_rate) / n)  # two sides frequency range\n",
    "    freq_array = freq_array[: (n // 2)]  # one side frequency range\n",
    "\n",
    "    raw_audio_data = raw_audio_data - np.average(raw_audio_data)  # zero-centering\n",
    "    freq_magnitude = scipy.fft(raw_audio_data)  # fft computing and normalization\n",
    "    freq_magnitude = freq_magnitude[: (n // 2)]  # one side\n",
    "\n",
    "    if max_frequency:\n",
    "        max_index = int(max_frequency * n / sample.frame_rate) + 1\n",
    "        freq_array = freq_array[:max_index]\n",
    "        freq_magnitude = freq_magnitude[:max_index]\n",
    "\n",
    "    freq_magnitude = abs(freq_magnitude)\n",
    "    freq_magnitude = freq_magnitude / np.sum(freq_magnitude)\n",
    "    return freq_array, freq_magnitude\n",
    "\n",
    "\n",
    "def classify_note_attempt_1(freq_array, freq_magnitude):\n",
    "    i = np.argmax(freq_magnitude)\n",
    "    f = freq_array[i]\n",
    "    print(\"frequency {}\".format(f))\n",
    "    print(\"magnitude {}\".format(freq_magnitude[i]))\n",
    "    return get_note_for_freq(f)\n",
    "\n",
    "\n",
    "def classify_note_attempt_2(freq_array, freq_magnitude):\n",
    "    note_counter = Counter()\n",
    "    for i in range(len(freq_magnitude)):\n",
    "        if freq_magnitude[i] < 0.01:\n",
    "            continue\n",
    "        note = get_note_for_freq(freq_array[i])\n",
    "        if note:\n",
    "            note_counter[note] += freq_magnitude[i]\n",
    "    return note_counter.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def classify_note_attempt_3(freq_array, freq_magnitude):\n",
    "    min_freq = 82\n",
    "    note_counter = Counter()\n",
    "    for i in range(len(freq_magnitude)):\n",
    "        if freq_magnitude[i] < 0.01:\n",
    "            continue\n",
    "\n",
    "        for freq_multiplier, credit_multiplier in [\n",
    "            (1, 1),\n",
    "            (1 / 3, 3 / 4),\n",
    "            (1 / 5, 1 / 2),\n",
    "            (1 / 6, 1 / 2),\n",
    "            (1 / 7, 1 / 2),\n",
    "        ]:\n",
    "            freq = freq_array[i] * freq_multiplier\n",
    "            if freq < min_freq:\n",
    "                continue\n",
    "            note = get_note_for_freq(freq)\n",
    "            if note:\n",
    "                note_counter[note] += freq_magnitude[i] * credit_multiplier\n",
    "\n",
    "    return note_counter.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "# If f is within tolerance of a note (measured in cents - 1/100th of a semitone)\n",
    "# return that note, otherwise returns None\n",
    "# We scale to the 440 octave to check\n",
    "def get_note_for_freq(f, tolerance=33):\n",
    "    # Calculate the range for each note\n",
    "    tolerance_multiplier = 2 ** (tolerance / 1200)\n",
    "    note_ranges = {\n",
    "        k: (v / tolerance_multiplier, v * tolerance_multiplier) for (k, v) in NOTES.items()\n",
    "    }\n",
    "\n",
    "    # Get the frequence into the 440 octave\n",
    "    range_min = note_ranges[\"A\"][0]\n",
    "    range_max = note_ranges[\"G#\"][1]\n",
    "    if f < range_min:\n",
    "        while f < range_min:\n",
    "            f *= 2\n",
    "    else:\n",
    "        while f > range_max:\n",
    "            f /= 2\n",
    "\n",
    "    # Check if any notes match\n",
    "    for (note, note_range) in note_ranges.items():\n",
    "        if f > note_range[0] and f < note_range[1]:\n",
    "            return note\n",
    "    return None\n",
    "\n",
    "\n",
    "# Assumes everything is either natural or sharp, no flats\n",
    "# Returns the Levenshtein distance between the actual notes and the predicted notes\n",
    "def calculate_distance(predicted, actual):\n",
    "    # To make a simple string for distance calculations we make natural notes lower case\n",
    "    # and sharp notes cap\n",
    "    def transform(note):\n",
    "        if \"#\" in note:\n",
    "            return note[0].upper()\n",
    "        return note.lower()\n",
    "\n",
    "    return distance(\n",
    "        \"\".join([transform(n) for n in predicted]), \"\".join([transform(n) for n in actual]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file, note_file=None, note_starts_file=None, plot_starts=False, plot_fft_indices=[]):\n",
    "    # If a note file and/or actual start times are supplied read them in\n",
    "    actual_starts = []\n",
    "    if note_starts_file:\n",
    "        with open(note_starts_file) as f:\n",
    "            for line in f:\n",
    "                actual_starts.append(float(line.strip()))\n",
    "\n",
    "    actual_notes = []\n",
    "    if note_file:\n",
    "        with open(note_file) as f:\n",
    "            for line in f:\n",
    "                actual_notes.append(line.strip())\n",
    "\n",
    "    song = AudioSegment.from_file(file)\n",
    "    song = song.high_pass_filter(80, order=4)\n",
    "\n",
    "    starts = predict_note_starts(song, plot_starts, actual_starts)\n",
    "\n",
    "    predicted_notes = predict_notes(song, starts, actual_notes, plot_fft_indices)\n",
    "\n",
    "    print(\"\")\n",
    "    if actual_notes:\n",
    "        print(\"Actual Notes\")\n",
    "        print(actual_notes)\n",
    "    print(\"Predicted Notes\")\n",
    "    print(predicted_notes)\n",
    "\n",
    "    if actual_notes:\n",
    "        lev_distance = calculate_distance(predicted_notes, actual_notes)\n",
    "        print(\"Levenshtein distance: {}/{}\".format(lev_distance, len(actual_notes)))\n",
    "    \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very simple implementation, just requires a minimum volume and looks for left edges by\n",
    "# comparing with the prior sample, also requires a minimum distance between starts\n",
    "# Future improvements could include smoothing and/or comparing multiple samples\n",
    "#\n",
    "# song: pydub.AudioSegment\n",
    "# plot: bool, whether to show a plot of start times\n",
    "# actual_starts: []float, time into song of each actual note start (seconds)\n",
    "#\n",
    "# Returns perdicted starts in ms\n",
    "def predict_note_starts(song, plot, actual_starts):\n",
    "    # Size of segments to break song into for volume calculations\n",
    "    SEGMENT_MS = 50\n",
    "    # Minimum volume necessary to be considered a note\n",
    "    VOLUME_THRESHOLD = -35\n",
    "    # The increase from one sample to the next required to be considered a note\n",
    "    EDGE_THRESHOLD = 5\n",
    "    # Throw out any additional notes found in this window\n",
    "    MIN_MS_BETWEEN = 100\n",
    "\n",
    "    # Filter out lower frequencies to reduce noise\n",
    "    song = song.high_pass_filter(80, order=4)\n",
    "    # dBFS is decibels relative to the maximum possible loudness\n",
    "    volume = [segment.dBFS for segment in song[::SEGMENT_MS]]\n",
    "\n",
    "    predicted_starts = []\n",
    "    for i in range(1, len(volume)):\n",
    "        if volume[i] > VOLUME_THRESHOLD and volume[i] - volume[i - 1] > EDGE_THRESHOLD:\n",
    "            ms = i * SEGMENT_MS\n",
    "            # Ignore any too close together\n",
    "            if len(predicted_starts) == 0 or ms - predicted_starts[-1] >= MIN_MS_BETWEEN:\n",
    "                predicted_starts.append(ms)\n",
    "\n",
    "    # If actual note start times are provided print a comparison\n",
    "    if len(actual_starts) > 0:\n",
    "        print(\"Approximate actual note start times ({})\".format(len(actual_starts)))\n",
    "        print(\" \".join([\"{:5.2f}\".format(s) for s in actual_starts]))\n",
    "        print(\"Predicted note start times ({})\".format(len(predicted_starts)))\n",
    "        print(\" \".join([\"{:5.2f}\".format(ms / 1000) for ms in predicted_starts]))\n",
    "\n",
    "    # Plot the volume over time (sec)\n",
    "    if plot:\n",
    "        x_axis = np.arange(len(volume)) * (SEGMENT_MS / 1000)\n",
    "        plt.plot(x_axis, volume)\n",
    "\n",
    "        # Add vertical lines for predicted note starts and actual note starts\n",
    "        for s in actual_starts:\n",
    "            plt.axvline(x=s, color=\"r\", linewidth=0.5, linestyle=\"-\")\n",
    "        for ms in predicted_starts:\n",
    "            plt.axvline(x=(ms / 1000), color=\"g\", linewidth=0.5, linestyle=\":\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return predicted_starts\n",
    "\n",
    "\n",
    "def predict_notes(song, starts, actual_notes, plot_fft_indices):\n",
    "    predicted_notes = []\n",
    "    for i, start in enumerate(starts):\n",
    "        sample_from = start + 50\n",
    "        sample_to = start + 550\n",
    "        if i < len(starts) - 1:\n",
    "            sample_to = min(starts[i + 1], sample_to)\n",
    "        segment = song[sample_from:sample_to]\n",
    "        freqs, freq_magnitudes = frequency_spectrum(segment)\n",
    "\n",
    "        predicted = classify_note_attempt_3(freqs, freq_magnitudes)\n",
    "        predicted_notes.append(predicted or \"U\")\n",
    "\n",
    "        # Print general info\n",
    "        print(\"\")\n",
    "        print(\"Note: {}\".format(i))\n",
    "        if i < len(actual_notes):\n",
    "            print(\"Predicted: {} Actual: {}\".format(predicted, actual_notes[i]))\n",
    "        else:\n",
    "            print(\"Predicted: {}\".format(predicted))\n",
    "        print(\"Predicted start: {}\".format(start))\n",
    "        length = sample_to - sample_from\n",
    "        print(\"Sampled from {} to {} ({} ms)\".format(sample_from, sample_to, length))\n",
    "        print(\"Frequency sample period: {}hz\".format(freqs[1]))\n",
    "\n",
    "        # Print peak info\n",
    "        peak_indicies, props = scipy.signal.find_peaks(freq_magnitudes, height=0.015)\n",
    "        print(\"Peaks of more than 1.5 percent of total frequency contribution:\")\n",
    "        for j, peak in enumerate(peak_indicies):\n",
    "            freq = freqs[peak]\n",
    "            magnitude = props[\"peak_heights\"][j]\n",
    "            print(\"{:.1f}hz with magnitude {:.3f}\".format(freq, magnitude))\n",
    "\n",
    "        if i in plot_fft_indices:\n",
    "            plt.plot(freqs, freq_magnitudes, \"b\")\n",
    "            plt.xlabel(\"Freq (Hz)\")\n",
    "            plt.ylabel(\"|X(freq)|\")\n",
    "            plt.show()\n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdd = main('eod.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATE NOTES FROM MIDI FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_data = pretty_midi.PrettyMIDI('EdgeofDesire.mid')\n",
    "midi_list = []\n",
    "\n",
    "for instrument in midi_data.instruments:\n",
    "    for note in instrument.notes:\n",
    "        start = note.start\n",
    "        end = note.end\n",
    "        pitch = note.pitch\n",
    "        velocity = note.velocity\n",
    "        midi_list.append([start, end, pitch, velocity, instrument.name])\n",
    "        \n",
    "midi_list = sorted(midi_list, key=lambda x: (x[0], x[2]))\n",
    "\n",
    "df = pd.DataFrame(midi_list, columns=['Start', 'End', 'Pitch', 'Velocity', 'Instrument'])\n",
    "html = df.to_html(index=False)\n",
    "ipd.HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = df['Pitch']\n",
    "freq = list(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2, pow\n",
    "\n",
    "A4 = 440\n",
    "C0 = A4*pow(2, -4.75)\n",
    "name = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "    \n",
    "def pitch(freq):\n",
    "    h = round(12*log2(freq/C0))\n",
    "    octave = h // 12\n",
    "    n = h % 12\n",
    "    return name[n] + str(octave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(freq)):\n",
    "    freq[i] = pitch(freq[i])\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHEET MUSIC(BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21 as m21\n",
    "\n",
    "s = m21.converter.parse('EdgeofDesire.mid')\n",
    "s.plot('pianoroll', figureSize=(24, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
